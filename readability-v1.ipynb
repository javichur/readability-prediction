{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73651bf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dda1156",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available() == False:\n",
    "    print('Error, selecciona una instancia con GPU.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26756900",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96a75abb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ec2-user/SageMaker'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83e39883",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER_BASE = './dataset'\n",
    "FOLDER_OUTPUT = './output'\n",
    "\n",
    "S3_BUCKET_NAME = 'sagemaker-readability'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7632ff74",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(FOLDER_OUTPUT):\n",
    "    os.makedirs(FOLDER_OUTPUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4eb5309a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q simpletransformers==0.51.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "176dbe91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ef29554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: transformers\n",
      "Version: 4.15.0\n",
      "Summary: State-of-the-art Natural Language Processing for TensorFlow 2.0 and PyTorch\n",
      "Home-page: https://github.com/huggingface/transformers\n",
      "Author: Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Sam Shleifer, Patrick von Platen, Sylvain Gugger, Suraj Patil, Stas Bekman, Google AI Language Team Authors, Open AI team Authors, Facebook AI Authors, Carnegie Mellon University Authors\n",
      "Author-email: thomas@huggingface.co\n",
      "License: Apache\n",
      "Location: /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages\n",
      "Requires: filelock, huggingface-hub, importlib-metadata, numpy, packaging, pyyaml, regex, requests, sacremoses, tokenizers, tqdm\n",
      "Required-by: simpletransformers\n"
     ]
    }
   ],
   "source": [
    "!pip show transformers # check transformers version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b17fc6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: torch\n",
      "Version: 1.8.1+cu111\n",
      "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
      "Home-page: https://pytorch.org/\n",
      "Author: PyTorch Team\n",
      "Author-email: packages@pytorch.org\n",
      "License: BSD-3\n",
      "Location: /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages\n",
      "Requires: numpy, typing-extensions\n",
      "Required-by: fastai, torchvision\n"
     ]
    }
   ],
   "source": [
    "!pip show torch # check version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "456538ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers_version='4.12.3' # la Ãºltima no es compatible con el SDK de Sagemaker. Usar 4.12.3.\n",
    "pytorch_version='1.9.1'\n",
    "py_version='py38'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8cc165e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOBAL_SEED = 33\n",
    "\n",
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    print('SEED: ' + str(seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2d3c8bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class experiment: \n",
    "  def __init__(self, SUBMISSION_NUM, FOLDER, FORCE_LOWERCASE, CONFIG_CLEAN_CHARS, REMOVE_0_0, REMOVE_STOP_WORDS, MODEL_TYPE, MODEL_NAME, AUTO_TOKENIZER_NAME, P_TEST_SIZE, P_TRAIN_SIZE, LOAD_TRAIN_AND_TEST_SETS, FILENAME_INPUT_SUFIX, NUM_TRAIN_EPOCHS, IS_FP16_ENABLED, MAX_SEQ_LENGTH, EVAL_BATCH_SIZE, TRAIN_BATCH_SIZE, CONFIG_GRADIENT_ACCUMULATION_STEPS, LEARNING_RATE, CONFIG_NO_SAVE_MODEL, WARMUP_STEPS=0): \n",
    "    self.FORCE_LOWERCASE = FORCE_LOWERCASE\n",
    "    self.CONFIG_CLEAN_CHARS = CONFIG_CLEAN_CHARS\n",
    "\n",
    "    self.REMOVE_0_0 = REMOVE_0_0\n",
    "\n",
    "    self.REMOVE_STOP_WORDS = REMOVE_STOP_WORDS\n",
    "\n",
    "    self.MODEL_TYPE = MODEL_TYPE # Ejemplos: 'bert'\n",
    "    self.MODEL_NAME = MODEL_NAME # Ejemplos: 'bert-large-uncased', 'bert-base-uncased','roberta-large'\n",
    "    self.AUTO_TOKENIZER_NAME = AUTO_TOKENIZER_NAME\n",
    "\n",
    "    self.P_TEST_SIZE = P_TEST_SIZE\n",
    "    self.P_TRAIN_SIZE = P_TRAIN_SIZE\n",
    "\n",
    "    self.LOAD_TRAIN_AND_TEST_SETS = LOAD_TRAIN_AND_TEST_SETS\n",
    "    self.FILENAME_INPUT_SUFIX = FILENAME_INPUT_SUFIX\n",
    "\n",
    "    self.NUM_TRAIN_EPOCHS = NUM_TRAIN_EPOCHS\n",
    "    self.IS_FP16_ENABLED = IS_FP16_ENABLED\n",
    "    self.MAX_SEQ_LENGTH = MAX_SEQ_LENGTH # Se rellena auto al tokenizar.\n",
    "    self.EVAL_BATCH_SIZE = EVAL_BATCH_SIZE\n",
    "    self.TRAIN_BATCH_SIZE = TRAIN_BATCH_SIZE\n",
    "    self.CONFIG_GRADIENT_ACCUMULATION_STEPS = CONFIG_GRADIENT_ACCUMULATION_STEPS\n",
    "    self.LEARNING_RATE = LEARNING_RATE # Ejemplos: 1e-05, 4e-05\n",
    "\n",
    "    self.CONFIG_NO_SAVE_MODEL = CONFIG_NO_SAVE_MODEL\n",
    "\n",
    "    self.WARMUP_STEPS = WARMUP_STEPS\n",
    "\n",
    "    now = datetime.now()\n",
    "    dt_string = now.strftime(\"%Y-%m-%d_%H_%M_%S\")\n",
    "    self.OUTPUT_FOLDER = os.path.join(FOLDER, 'out-' + MODEL_NAME + '_' + str(SUBMISSION_NUM) + '_' + dt_string)\n",
    "\n",
    "  def info(self):\n",
    "    return self.MODEL_NAME + '-N' + str(self.NUM_TRAIN_EPOCHS) + '_LR' + str(self.LEARNING_RATE) + '_lower' + str(self.FORCE_LOWERCASE) + '_cleanChars' + str(self.CONFIG_CLEAN_CHARS) + '_removeStops' + str(self.REMOVE_STOP_WORDS) + '_removeZero' + str(self.REMOVE_0_0) + '_warm' + str(self.WARMUP_STEPS) + '_' + str(self.P_TRAIN_SIZE) + '_' + str(self.TRAIN_BATCH_SIZE) + 'x' + str(self.EVAL_BATCH_SIZE) + '_' + str(self.MAX_SEQ_LENGTH) + '_fp16' + str(self.IS_FP16_ENABLED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ddc1d123",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9d4f852e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig = pd.read_csv(os.path.join(FOLDER_BASE, 'train.csv'), sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9a734d77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url_legal</th>\n",
       "      <th>license</th>\n",
       "      <th>excerpt</th>\n",
       "      <th>target</th>\n",
       "      <th>standard_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c12129c31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>When the young people returned to the ballroom...</td>\n",
       "      <td>-0.340259</td>\n",
       "      <td>0.464009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85aa80a4c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All through dinner time, Mrs. Fayre was somewh...</td>\n",
       "      <td>-0.315372</td>\n",
       "      <td>0.480805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b69ac6792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As Roger had predicted, the snow departed as q...</td>\n",
       "      <td>-0.580118</td>\n",
       "      <td>0.476676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dd1000b26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>And outside before the palace a great garden w...</td>\n",
       "      <td>-1.054013</td>\n",
       "      <td>0.450007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37c1b32fb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Once upon a time there were Three Bears who li...</td>\n",
       "      <td>0.247197</td>\n",
       "      <td>0.510845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2829</th>\n",
       "      <td>25ca8f498</td>\n",
       "      <td>https://sites.ehe.osu.edu/beyondpenguins/files...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>When you think of dinosaurs and where they liv...</td>\n",
       "      <td>1.711390</td>\n",
       "      <td>0.646900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2830</th>\n",
       "      <td>2c26db523</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Wikijunior:The_E...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>So what is a solid? Solids are usually hard be...</td>\n",
       "      <td>0.189476</td>\n",
       "      <td>0.535648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2831</th>\n",
       "      <td>cd19e2350</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Wikijunior:The_E...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>The second state of matter we will discuss is ...</td>\n",
       "      <td>0.255209</td>\n",
       "      <td>0.483866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2832</th>\n",
       "      <td>15e2e9e7a</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Geometry_for_Ele...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>Solids are shapes that you can actually touch....</td>\n",
       "      <td>-0.215279</td>\n",
       "      <td>0.514128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2833</th>\n",
       "      <td>5b990ba77</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Wikijunior:Biolo...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>Animals are made of many cells. They eat thing...</td>\n",
       "      <td>0.300779</td>\n",
       "      <td>0.512379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2834 rows Ã 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                          url_legal  \\\n",
       "0     c12129c31                                                NaN   \n",
       "1     85aa80a4c                                                NaN   \n",
       "2     b69ac6792                                                NaN   \n",
       "3     dd1000b26                                                NaN   \n",
       "4     37c1b32fb                                                NaN   \n",
       "...         ...                                                ...   \n",
       "2829  25ca8f498  https://sites.ehe.osu.edu/beyondpenguins/files...   \n",
       "2830  2c26db523  https://en.wikibooks.org/wiki/Wikijunior:The_E...   \n",
       "2831  cd19e2350  https://en.wikibooks.org/wiki/Wikijunior:The_E...   \n",
       "2832  15e2e9e7a  https://en.wikibooks.org/wiki/Geometry_for_Ele...   \n",
       "2833  5b990ba77  https://en.wikibooks.org/wiki/Wikijunior:Biolo...   \n",
       "\n",
       "           license                                            excerpt  \\\n",
       "0              NaN  When the young people returned to the ballroom...   \n",
       "1              NaN  All through dinner time, Mrs. Fayre was somewh...   \n",
       "2              NaN  As Roger had predicted, the snow departed as q...   \n",
       "3              NaN  And outside before the palace a great garden w...   \n",
       "4              NaN  Once upon a time there were Three Bears who li...   \n",
       "...            ...                                                ...   \n",
       "2829  CC BY-SA 3.0  When you think of dinosaurs and where they liv...   \n",
       "2830  CC BY-SA 3.0  So what is a solid? Solids are usually hard be...   \n",
       "2831  CC BY-SA 3.0  The second state of matter we will discuss is ...   \n",
       "2832  CC BY-SA 3.0  Solids are shapes that you can actually touch....   \n",
       "2833  CC BY-SA 3.0  Animals are made of many cells. They eat thing...   \n",
       "\n",
       "        target  standard_error  \n",
       "0    -0.340259        0.464009  \n",
       "1    -0.315372        0.480805  \n",
       "2    -0.580118        0.476676  \n",
       "3    -1.054013        0.450007  \n",
       "4     0.247197        0.510845  \n",
       "...        ...             ...  \n",
       "2829  1.711390        0.646900  \n",
       "2830  0.189476        0.535648  \n",
       "2831  0.255209        0.483866  \n",
       "2832 -0.215279        0.514128  \n",
       "2833  0.300779        0.512379  \n",
       "\n",
       "[2834 rows x 6 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cb0f2436",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBMISSION_ID = 1\n",
    "\n",
    "aux_experimento = experiment(\n",
    "  SUBMISSION_NUM = SUBMISSION_ID,\n",
    "  FOLDER = FOLDER_OUTPUT,\n",
    "  \n",
    "  FORCE_LOWERCASE = False,\n",
    "  CONFIG_CLEAN_CHARS = False,\n",
    "\n",
    "  REMOVE_0_0 = False,\n",
    "  REMOVE_STOP_WORDS = False,\n",
    "\n",
    "  MODEL_TYPE = 'roberta', # 'bert', 'roberta'\n",
    "  MODEL_NAME = 'roberta-base', # '../input/roberta-base', # 'roberta-base', # 'bert-large-uncased' # 'bert-base-uncased' #roberta-large'\n",
    "  AUTO_TOKENIZER_NAME = 'roberta-base', # '../input/roberta-base', # igual que model_name\n",
    "\n",
    "  P_TEST_SIZE = 0.1,\n",
    "  P_TRAIN_SIZE = 0.9,\n",
    "  LOAD_TRAIN_AND_TEST_SETS = False,\n",
    "  FILENAME_INPUT_SUFIX = 'writing-in-the-blog',\n",
    "\n",
    "  NUM_TRAIN_EPOCHS = 5,\n",
    "  IS_FP16_ENABLED = False,\n",
    "  MAX_SEQ_LENGTH = 10000, # Se rellena auto al tokenizar.\n",
    "  EVAL_BATCH_SIZE = 8,\n",
    "  TRAIN_BATCH_SIZE = 8,\n",
    "  CONFIG_GRADIENT_ACCUMULATION_STEPS = 1,\n",
    "  LEARNING_RATE = 4e-5,\n",
    "\n",
    "  WARMUP_STEPS = 0,\n",
    "\n",
    "  CONFIG_NO_SAVE_MODEL = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "943e75c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "import logging\n",
    "\n",
    "def prediction_error(labels, preds):\n",
    "  errors = [\n",
    "             abs(label - pred)\n",
    "             for label, pred in zip(labels, preds)\n",
    "  ]\n",
    "  return statistics.mean(errors)\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8ee4e611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEED: 33\n"
     ]
    }
   ],
   "source": [
    "seed_everything(seed=GLOBAL_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8a496bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Separado el dataset en train y test. Guardado en disco.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ea879be8b1942edaba06a3cc06cb693",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12e5685178ea4556987de57dbd17f9a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "433a89631eb948bbb01f19b2da6e15b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65ff4f9a441e488599ea9269d10116cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX_SEQ_LENGTH (from Train & Eval): 322\n",
      "==> 1: roberta-base-N5_LR4e-05_lowerFalse_cleanCharsFalse_removeStopsFalse_removeZeroFalse_warm0_0.9_8x8_322_fp16False\n"
     ]
    }
   ],
   "source": [
    "one = aux_experimento\n",
    "\n",
    "if one.LOAD_TRAIN_AND_TEST_SETS == False:\n",
    "  x_train, x_test = train_test_split(df_orig, test_size=one.P_TEST_SIZE, train_size=one.P_TRAIN_SIZE, shuffle=True)\n",
    "\n",
    "  now = datetime.now()\n",
    "  dt_string = now.strftime(\"%Y-%m-%d_%H_%M_%S\")\n",
    "  suffix = dt_string + '_' + str(one.P_TRAIN_SIZE) + 'x' + str(one.P_TEST_SIZE)\n",
    "    \n",
    "  x_train.to_csv(os.path.join(FOLDER_OUTPUT, \"x_train_\" + suffix +  \".csv\"), sep=\",\", index=False)\n",
    "  x_test.to_csv(os.path.join(FOLDER_OUTPUT, \"x_test_\" + suffix  + \".csv\"), sep=\",\", index=False)\n",
    "\n",
    "  print('Separado el dataset en train y test. Guardado en disco.')\n",
    "else:\n",
    "  x_train = pd.read_csv(os.path.join(FOLDER_OUTPUT, 'x_train_' + one.FILENAME_INPUT_SUFIX + \".csv\"), sep=\",\", error_bad_lines=True)\n",
    "  x_test = pd.read_csv(os.path.join(FOLDER_OUTPUT, 'x_test_' + one.FILENAME_INPUT_SUFIX + \".csv\"), sep=\",\", error_bad_lines=True)\n",
    "  print('Se ha cargado de disco x_train y x_test.')\n",
    "  \n",
    "# eliminar el dato anÃ³malo?:\n",
    "if one.REMOVE_0_0:\n",
    "  x_train = x_train[(x_train['target'] != 0) | (x_train['standard_error'] != 0)]\n",
    "  x_test = x_test[(x_test['target'] != 0) | (x_test['standard_error'] != 0)]\n",
    "  print('Eliminado dato anÃ³malo (0, 0). ' + str(len(x_train)))\n",
    "\n",
    "x_train.rename(columns = {'excerpt':'text', 'target':'labels'}, inplace = True)\n",
    "x_test.rename(columns = {'excerpt':'text', 'target':'labels'}, inplace = True)\n",
    "\n",
    "if one.FORCE_LOWERCASE == True:\n",
    "  x_train['text'] = x_train['text'].str.lower()\n",
    "  x_test['text'] = x_test['text'].str.lower()\n",
    "  print('ConversiÃ³n a minÃºsculas.')\n",
    "\n",
    "if one.CONFIG_CLEAN_CHARS:\n",
    "  x_train['text'] = x_train['text'].apply(lambda x: clean_chars(x, stops_spaced_only))\n",
    "  x_test['text'] = x_test['text'].apply(lambda x: clean_chars(x, stops_spaced_only))\n",
    "  print('Limpiados los caracteres especiales.')\n",
    "\n",
    "if one.REMOVE_STOP_WORDS:\n",
    "  print('Limpiadas las stopwords.')\n",
    "  for one in stops:\n",
    "    x_train['text'] = x_train['text'].apply(lambda x: x.replace(' {} '.format(one), ' '))\n",
    "    x_test['text'] = x_test['text'].apply(lambda x: x.replace(' {} '.format(one), ' '))\n",
    "\n",
    "# calcular max_seq\n",
    "tokenizer = AutoTokenizer.from_pretrained(one.AUTO_TOKENIZER_NAME)\n",
    "\n",
    "x_train['excerpt_tokeniker'] = x_train['text'].apply(lambda x: tokenizer.tokenize(x))\n",
    "x_train['excerpt_tokeniker_len'] = x_train['excerpt_tokeniker'].apply(lambda x: len(x))\n",
    "\n",
    "x_test['excerpt_tokeniker'] = x_test['text'].apply(lambda x: tokenizer.tokenize(x))\n",
    "x_test['excerpt_tokeniker_len'] = x_test['excerpt_tokeniker'].apply(lambda x: len(x))\n",
    "\n",
    "one.MAX_SEQ_LENGTH = max(x_train['excerpt_tokeniker_len'].max(), x_test['excerpt_tokeniker_len'].max()) + 2 # max len de Train y Eval. (y +2)\n",
    "print('MAX_SEQ_LENGTH (from Train & Eval): ' + str(one.MAX_SEQ_LENGTH))\n",
    "\n",
    "\n",
    "print('==> ' + str(SUBMISSION_ID) + ': ' + one.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0103ffa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassificationArgs(adam_epsilon=1e-08, best_model_dir='./output/out-roberta-base_1_2022-01-09_19_42_25/best_model', cache_dir='cache_dir/', config={}, cosine_schedule_num_cycles=0.5, custom_layer_parameters=[], custom_parameter_groups=[], dataloader_num_workers=2, do_lower_case=False, dynamic_quantize=False, early_stopping_consider_epochs=False, early_stopping_delta=0, early_stopping_metric='eval_loss', early_stopping_metric_minimize=True, early_stopping_patience=3, encoding=None, adafactor_eps=(1e-30, 0.001), adafactor_clip_threshold=1.0, adafactor_decay_rate=-0.8, adafactor_beta1=None, adafactor_scale_parameter=True, adafactor_relative_step=True, adafactor_warmup_init=True, eval_batch_size=8, evaluate_during_training=True, evaluate_during_training_silent=True, evaluate_during_training_steps=2000, evaluate_during_training_verbose=True, evaluate_each_epoch=True, fp16=False, gradient_accumulation_steps=1, learning_rate=4e-05, local_rank=-1, logging_steps=50, manual_seed=33, max_grad_norm=1.0, max_seq_length=322, model_name=None, model_type=None, multiprocessing_chunksize=500, n_gpu=1, no_cache=False, no_save=False, not_saved_args=[], num_train_epochs=5, optimizer='AdamW', output_dir='./output/out-roberta-base_1_2022-01-09_19_42_25', overwrite_output_dir=False, process_count=2, polynomial_decay_schedule_lr_end=1e-07, polynomial_decay_schedule_power=1.0, quantized_model=False, reprocess_input_data=True, save_best_model=True, save_eval_checkpoints=True, save_model_every_epoch=True, save_optimizer_and_scheduler=True, save_steps=2000, scheduler='linear_schedule_with_warmup', silent=False, skip_special_tokens=True, tensorboard_dir=None, thread_count=None, train_batch_size=8, train_custom_parameters_only=False, use_cached_eval_features=False, use_early_stopping=False, use_multiprocessing=True, wandb_kwargs={}, wandb_project=None, warmup_ratio=0.06, warmup_steps=0, weight_decay=0.0, model_class='ClassificationModel', labels_list=[], labels_map={}, lazy_delimiter='\\t', lazy_labels_column=1, lazy_loading=False, lazy_loading_start_line=1, lazy_text_a_column=None, lazy_text_b_column=None, lazy_text_column=0, onnx=False, regression=True, sliding_window=False, stride=0.8, tie_value=1)\n"
     ]
    }
   ],
   "source": [
    "model_args = ClassificationArgs()\n",
    "model_args.num_train_epochs = one.NUM_TRAIN_EPOCHS\n",
    "model_args.output_dir = one.OUTPUT_FOLDER\n",
    "model_args.regression = True\n",
    "model_args.evaluate_during_training = True\n",
    "model_args.evaluate_during_training_verbose = True\n",
    "\n",
    "model_args.fp16 = one.IS_FP16_ENABLED\n",
    "model_args.max_seq_length = one.MAX_SEQ_LENGTH\n",
    "model_args.eval_batch_size = one.EVAL_BATCH_SIZE\n",
    "model_args.train_batch_size = one.TRAIN_BATCH_SIZE\n",
    "model_args.gradient_accumulation_steps = one.CONFIG_GRADIENT_ACCUMULATION_STEPS\n",
    "\n",
    "model_args.learning_rate = one.LEARNING_RATE\n",
    "\n",
    "model_args.no_save = one.CONFIG_NO_SAVE_MODEL\n",
    "model_args.best_model_dir = os.path.join(one.OUTPUT_FOLDER, 'best_model')\n",
    "\n",
    "model_args.manual_seed = GLOBAL_SEED # para reproducible\n",
    "\n",
    "model_args.warmup_steps = one.WARMUP_STEPS\n",
    "\n",
    "# model_args.wandb_project = WANDB_PROJECT_NAME\n",
    "# model_args.wandb_kwargs['name'] = one.info()\n",
    "\n",
    "print(model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "54170e55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8e3698a1d9646058606144934f131f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/478M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO:simpletransformers.classification.classification_model: Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa74262fc58148fbac0528211c79f474",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2550 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e317977e70744e6a04632e2d16813d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed841d7143794eb4972d81e015cd736f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 0 of 5:   0%|          | 0/319 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_model: Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_model:{'prediction_error': 0.4972742662001008, 'eval_loss': 0.37080445513129234}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9549bb819954b99b51915edc558c2fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 1 of 5:   0%|          | 0/319 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_model: Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_model:{'prediction_error': 0.5295451188551813, 'eval_loss': 0.42812980131970513}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0b1410134d746fdbbfd7194ac3e535a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 2 of 5:   0%|          | 0/319 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_model: Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_model:{'prediction_error': 0.6533960971832079, 'eval_loss': 0.6184557212723626}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad7de7571bcc429994979c687c0cb530",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 3 of 5:   0%|          | 0/319 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_model: Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_model:{'prediction_error': 0.4459620239672212, 'eval_loss': 0.3230808104077975}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14cd83313e8549daa2fa224c595276df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 4 of 5:   0%|          | 0/319 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_model: Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_model:{'prediction_error': 0.47813418899616883, 'eval_loss': 0.36507454328238964}\n",
      "INFO:simpletransformers.classification.classification_model: Training of roberta model complete. Saved to ./output/out-roberta-base_1_2022-01-09_19_42_25.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1595,\n",
       " {'global_step': [319, 638, 957, 1276, 1595],\n",
       "  'train_loss': [0.3251998722553253,\n",
       "   0.07599388808012009,\n",
       "   0.3134761154651642,\n",
       "   0.0371391735970974,\n",
       "   0.12416594475507736],\n",
       "  'eval_loss': [0.37080445513129234,\n",
       "   0.42812980131970513,\n",
       "   0.6184557212723626,\n",
       "   0.3230808104077975,\n",
       "   0.36507454328238964],\n",
       "  'prediction_error': [0.4972742662001008,\n",
       "   0.5295451188551813,\n",
       "   0.6533960971832079,\n",
       "   0.4459620239672212,\n",
       "   0.47813418899616883]})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ClassificationModel(\n",
    "    one.MODEL_TYPE,\n",
    "    one.MODEL_NAME,\n",
    "    num_labels=1,\n",
    "    args=model_args\n",
    ")\n",
    "\n",
    "model.train_model(x_train, eval_df=x_test, show_running_loss=True, verbose=True, prediction_error=prediction_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "55196fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_model: Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83246a70065a48338395410b7c256df7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/284 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a9ca28c292b4cfabea98f820873b89f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Evaluation:   0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_model:{'prediction_error': 0.47813418899616883, 'eval_loss': 0.36507454328238964}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.36507454328238964}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result, model_outputs, wrong_predictions = model.eval_model(x_test, verbose=True)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4d1b8736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ABS) Mean prediction error for this experiment: 0.4459620252154145\n"
     ]
    }
   ],
   "source": [
    "STR_SUBMISSION_ID = str(SUBMISSION_ID)\n",
    "\n",
    "solutions = pd.DataFrame()\n",
    "solutions['text_' + STR_SUBMISSION_ID] = x_test['text']\n",
    "solutions['labels_' + STR_SUBMISSION_ID] = x_test['labels']\n",
    "solutions['standard_error_' + STR_SUBMISSION_ID] = x_test['standard_error']\n",
    "\n",
    "solutions['prediction_' + STR_SUBMISSION_ID] = model_outputs\n",
    "solutions['prediction_error_abs_' + STR_SUBMISSION_ID] = solutions.apply(lambda x: abs(x['labels_' + STR_SUBMISSION_ID] - x['prediction_' + STR_SUBMISSION_ID]), axis=1)\n",
    "\n",
    "print('(ABS) Mean prediction error for this experiment: ' + str(solutions['prediction_error_abs_' + STR_SUBMISSION_ID].mean(axis=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfea327",
   "metadata": {},
   "source": [
    "## Cargamos el modelo entrenado previamente guardado en la instancia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "714ac45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargando desde fichero el modelo entrenado\n",
    "model_loaded = ClassificationModel(\n",
    "    one.MODEL_TYPE,\n",
    "    os.path.join(one.OUTPUT_FOLDER, 'best_model'),\n",
    "    num_labels=1,\n",
    "    args=model_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ccaec80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_model: Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cc5f5583ad04778b9052f57ffba7c42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/284 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "952b188467f64eaf993aa4f1bdb285ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Evaluation:   0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_model:{'prediction_error': 0.4459620239672212, 'eval_loss': 0.3230808104077975}\n"
     ]
    }
   ],
   "source": [
    "result, model_outputs, wrong_predictions = model_loaded.eval_model(x_test, verbose=True, prediction_error=prediction_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f37a786a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prediction_error': 0.4459620239672212, 'eval_loss': 0.3230808104077975}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c0d3fd1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_model: Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "650f167b07b44d86b4e434d530caf58b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e74796dc3144140998bc6a4a8fef0b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.47816357, -2.13515425, -0.16586283])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ejemplos de predicciÃ³n:\n",
    "\n",
    "samples = ['A cat is an animal.',\n",
    "           'Quantum computing is a type of computation that harnesses the collective properties of quantum states, such as superposition, interference, and entanglement, to perform calculations.',\n",
    "           'The United States (U.S. or US), officially the United States of America (U.S.A. or USA) or America, is a country primarily located in North America.']\n",
    "           \n",
    "predictions, raw_outputs = model_loaded.predict(samples)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4223c931",
   "metadata": {},
   "source": [
    "## Enviamos el modelo entrenado a S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b015640f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./output/out-roberta-base_1_2022-01-09_19_42_25/best_model'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3, os\n",
    "\n",
    "BEST_DIR = os.path.join(one.OUTPUT_FOLDER, 'best_model')\n",
    "TARGZ_NAME = 'best_model.tar.gz'\n",
    "BEST_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2d2da45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpciÃ³n 1: enviar todos los ficheros, descomprimidos.\n",
    "# for (dirpath, dirnames, filenames) in os.walk(BEST_DIR):\n",
    "#    for one in filenames:\n",
    "#        path = os.path.join(dirpath, one)\n",
    "#        print('enviando a S3: ' + path)\n",
    "#        boto3.Session().resource('s3').Bucket(S3_BUCKET_NAME).Object(path).upload_file(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpciÃ³n 2: comprimir y enviar el comprimido\n",
    "%cd ~/SageMaker/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd {BEST_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd8ca04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.json\n",
      "eval_results.txt\n",
      "merges.txt\n",
      "model_args.json\n",
      "optimizer.pt\n",
      "pytorch_model.bin\n",
      "scheduler.pt\n",
      "special_tokens_map.json\n",
      "tokenizer_config.json\n",
      "training_args.bin\n",
      "vocab.json\n",
      "Comprimido :)\n",
      "Subido a S3 el tar.gz :)\n"
     ]
    }
   ],
   "source": [
    "!tar -zcvf {TARGZ_NAME} *\n",
    "\n",
    "print('Comprimido :)')\n",
    "\n",
    "boto3.Session().resource('s3').Bucket(S3_BUCKET_NAME).Object(TARGZ_NAME).upload_file(TARGZ_NAME)\n",
    "\n",
    "print('Subido a S3 el tar.gz :)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "658391d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-readability/best_model.tar.gz'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_data_url = 's3://' + S3_BUCKET_NAME + '/' + TARGZ_NAME\n",
    "model_data_url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad335f1",
   "metadata": {},
   "source": [
    "## A partir de aquÃ­, creamos el endpoint para inferencia serverless\n",
    "\n",
    "Basado en el ejemplo de Julien Simon (Chief Evangelist @ Hugging Face): https://dev.to/juliensimon/aws-reinvent-2021-serverless-inference-on-sagemaker-for-real-g96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d15a7634",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = boto3.client(service_name='sagemaker')\n",
    "\n",
    "from time import gmtime, strftime\n",
    "\n",
    "def name_with_timestamp(name):\n",
    "    return '{}-{}'.format(name, strftime('%Y-%m-%d-%H-%M-%S', gmtime()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8a132a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_model_name    = name_with_timestamp('huggingface-serverless')\n",
    "huggingface_epc_name      = name_with_timestamp('huggingface-serverless-epc')\n",
    "huggingface_endpoint_name = name_with_timestamp('huggingface-serverless-ep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "88a3cad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2f9ef7",
   "metadata": {},
   "source": [
    "### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "207636ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'763104351884.dkr.ecr.eu-west-1.amazonaws.com/huggingface-pytorch-inference:1.9.1-transformers4.12.3-cpu-py38-ubuntu20.04'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "region = boto3.session.Session().region_name\n",
    "\n",
    "image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework='huggingface',\n",
    "    base_framework_version=f'pytorch{pytorch_version}',\n",
    "    region=region,\n",
    "    version=transformers_version,\n",
    "    py_version=py_version,\n",
    "    instance_type='ml.m5.large',   # No GPU support on serverless inference\n",
    "    image_scope='inference'\n",
    ")\n",
    "\n",
    "image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4f4603c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arn:aws:sagemaker:eu-west-1:587943841427:model/huggingface-serverless-2022-01-09-20-15-26'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "create_model_response = sm.create_model(\n",
    "    ModelName=huggingface_model_name,\n",
    "    Containers=[\n",
    "        {\n",
    "            'Image': image_uri,\n",
    "            'Mode': 'SingleModel',\n",
    "            'ModelDataUrl': model_data_url,\n",
    "            #'Environment': {\n",
    "            #    'HF_TASK': 'text-classification',\n",
    "            #    'SAGEMAKER_ENABLE_CLOUDWATCH_METRICS': 'true',\n",
    "            #    'SAGEMAKER_CONTAINER_LOG_LEVEL': '20'\n",
    "            #}\n",
    "        }\n",
    "    ],\n",
    "    ExecutionRoleArn=role,\n",
    ")\n",
    "\n",
    "create_model_response[\"ModelArn\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09382eab",
   "metadata": {},
   "source": [
    "### Create endpoint configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "aec50e40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arn:aws:sagemaker:eu-west-1:587943841427:endpoint-config/huggingface-serverless-epc-2022-01-09-20-15-26'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint_config_response = sm.create_endpoint_config(\n",
    "    EndpointConfigName=huggingface_epc_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            'VariantName': 'single-variant',\n",
    "            'ModelName': huggingface_model_name,\n",
    "            \n",
    "            # opciÃ³n serverless:\n",
    "            'ServerlessConfig': {\n",
    "                'MemorySizeInMB': 6144,\n",
    "                'MaxConcurrency': 8,\n",
    "            },\n",
    "            \n",
    "            # opciÃ³n no serverless:\n",
    "            #'InstanceType': 'ml.t2.xlarge',\n",
    "            #'InitialInstanceCount': 1,\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "endpoint_config_response['EndpointConfigArn']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfd0b38",
   "metadata": {},
   "source": [
    "### Create endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9ec046f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arn:aws:sagemaker:eu-west-1:587943841427:endpoint/huggingface-serverless-ep-2022-01-09-20-15-26'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " create_endpoint_response = sm.create_endpoint(\n",
    "    EndpointName=huggingface_endpoint_name,\n",
    "    EndpointConfigName=huggingface_epc_name,\n",
    ")\n",
    "\n",
    "create_endpoint_response['EndpointArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d66b2d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "waiter = sm.get_waiter('endpoint_in_service')\n",
    "waiter.wait(EndpointName=huggingface_endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03345f8c",
   "metadata": {},
   "source": [
    "### Invoke endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "cebca3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading, time, json\n",
    "\n",
    "sm_rt = boto3.client(service_name='sagemaker-runtime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f2c313d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = { 'inputs': 'A cat is an animal.' }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3977f5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModelError",
     "evalue": "An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from primary with message \"{\n  \"code\": 400,\n  \"type\": \"InternalServerException\",\n  \"message\": \"No such file or directory (os error 2)\"\n}\n\". See https://eu-west-1.console.aws.amazon.com/cloudwatch/home?region=eu-west-1#logEventViewer:group=/aws/sagemaker/Endpoints/huggingface-serverless-ep-2022-01-10-17-21-20 in account 587943841427 for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mModelError\u001b[0m                                Traceback (most recent call last)\n",
      "\u001b[0;32m/tmp/ipykernel_10909/2354317780.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[1;32m      3\u001b[0m             \u001b[0mEndpointName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhuggingface_endpoint_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m      4\u001b[0m             \u001b[0mBody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m----> 5\u001b[0;31m             \u001b[0mContentType\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'application/json'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m      6\u001b[0m )\n",
      "\u001b[1;32m      7\u001b[0m \u001b[0mtock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    389\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n",
      "\u001b[1;32m    390\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m--> 391\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    393\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n",
      "\u001b[1;32m    717\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    718\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m--> 719\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m    720\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    721\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[0;31mModelError\u001b[0m: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from primary with message \"{\n",
      "  \"code\": 400,\n",
      "  \"type\": \"InternalServerException\",\n",
      "  \"message\": \"No such file or directory (os error 2)\"\n",
      "}\n",
      "\". See https://eu-west-1.console.aws.amazon.com/cloudwatch/home?region=eu-west-1#logEventViewer:group=/aws/sagemaker/Endpoints/huggingface-serverless-ep-2022-01-10-17-21-20 in account 587943841427 for more information."
     ]
    }
   ],
   "source": [
    "tick = time.time()\n",
    "response = sm_rt.invoke_endpoint(\n",
    "            EndpointName=huggingface_endpoint_name,\n",
    "            Body=json.dumps(test_data),\n",
    "            ContentType='application/json'\n",
    ")\n",
    "tock = time.time()\n",
    "print(tock-tick)\n",
    "# print(response[\"Body\"].read())\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86eddf9",
   "metadata": {},
   "source": [
    "### La invocaciÃ³n anterior falla.\n",
    "\n",
    "2022-01-09: En este hilo comentan que puede dar este error cuando el modelo > 512MB:\n",
    "https://discuss.huggingface.co/t/sagemaker-serverless-inference/13246\n",
    "\n",
    "2022-01-10: He corregido un error en la creaciÃ³n del fichero `best_model.tar.gz` (se estaba comprimiendo la carpeta y hay que comprimir los ficheros del modelo pero sin la carpeta). AÃºn asÃ­, ahora la invocaciÃ³n da otro error. He probado a cambiar el Endpoint Serverless por un Endpoint no Serverless y el error era el mismo. Finalmente he llegado a la conclusiÃ³n de que el modelo que guarda la librerÃ­a SimpleTransformers no es compatible con el modelo que lee la librerÃ­a Transformers (HuggingFace). Existen 2 posibles soluciones:\n",
    "\n",
    "1. En lugar de utilizar una imagen de contenedor de HuggingFace, crear una imagen personalizada para el contenedor de inferencia, que cargue el modelo y realice la predicciÃ³n con la librerÃ­a SimpleTransformers. DocumentaciÃ³n: https://docs.aws.amazon.com/sagemaker/latest/dg/adapt-inference-container.html\n",
    "\n",
    "2. Reimplementar el entrenamiento del modelo para usar la librerÃ­a Transformers (HuggingFace), en lugar de SimpleTransformers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404a179a",
   "metadata": {},
   "source": [
    "### Por Ãºltimo, eliminamos el endpoint de inferencia que habÃ­amos creado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d59d3205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'cce67e18-69be-45f5-a3d0-cde696f71523',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'cce67e18-69be-45f5-a3d0-cde696f71523',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '0',\n",
       "   'date': 'Sun, 09 Jan 2022 20:41:33 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm.delete_endpoint(EndpointName=huggingface_endpoint_name)\n",
    "sm.delete_endpoint_config(EndpointConfigName=huggingface_epc_name)\n",
    "sm.delete_model(ModelName=huggingface_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2944c3e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  },
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p37",
   "language": "python",
   "name": "conda_pytorch_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
